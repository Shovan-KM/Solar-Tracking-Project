# -*- coding: utf-8 -*-
"""An Adaptive and Predictive Intelligent Solar Energy System

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c3YUN5CbFERwJo-47KtfAtlaXTcH-cr4

# **Libraries**
"""

# libraries
import numpy as np
import pandas as pd
import seaborn as sns
from google.colab import files
import matplotlib.pyplot as plt
#sklearn
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_squared_error, confusion_matrix, classification_report

from statsmodels.tsa.seasonal import seasonal_decompose

from xgboost import XGBRegressor
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score

from imblearn.over_sampling import SMOTE

"""# **File Uploading**"""

# uploading Excel file
uploaded = files.upload()
#reading the file
for filename in uploaded.keys():
    df = pd.read_excel(filename)

df.columns = ['Timestamp', 'LDR_FL', 'LDR_FR', 'LDR_BL', 'LDR_BR', 'Voltage_Output', 'Energy_Generated', 'Pump_Status']

#print all the column names
print(df.columns)

"""# **Data Preprocessing:**"""

df.describe()

# Checking for null values
df.isnull().sum()

df.columns = ['Timestamp', 'LDR_FL', 'LDR_FR', 'LDR_BL', 'LDR_BR', 'Voltage_Output', 'Energy_Generated', 'Pump_Status']

# Converting Timestamp column to datetime
df['Timestamp'] = pd.to_datetime(df['Timestamp'])

# Feature scaling for LDR sensor readings
scaler = StandardScaler()
features_to_scale = ['LDR_FL', 'LDR_FR', 'LDR_BL', 'LDR_BR']
df[features_to_scale] = scaler.fit_transform(df[features_to_scale])

# Converting Pump_Status to binary (1 for 'ON', 0 for 'OFF')
df['Pump_Status_Binary'] = df['Pump_Status'].apply(lambda x: 1 if x == 'ON' else 0)

"""# **General EDA**"""

# Histograms
df.hist(figsize=(30, 10))
plt.tight_layout()
plt.show()

# Boxplots
sns.boxplot(x=df['Voltage_Output'])
plt.show()

# Correlation matrix heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Feature Correlation Heatmap')
plt.show()

# Distribution of our pump status (0 for OFF, 1 for ON)

sns.countplot(x='Pump_Status_Binary', data=df)
plt.title('Pump Status Distribution')
plt.show()

# Time series plot for voltage output visualization

plt.figure(figsize=(15, 5))
plt.plot(df['Timestamp'], df['Voltage_Output'], marker='o', linestyle='-')
plt.title('Voltage Output Over Time')
plt.xlabel('Timestamp')
plt.ylabel('Voltage Output')
plt.show()

#Pairplot for relationships between variable

sns.pairplot(df[['LDR_FL', 'LDR_FR', 'LDR_BL', 'LDR_BR', 'Voltage_Output', 'Energy_Generated', 'Pump_Status_Binary']])
plt.show()

"""# **Feature** **Engineering**"""

# Feature Engineering

# Hour and minute as a combined feature
df['TimeOfDay'] = df['Timestamp'].dt.hour + df['Timestamp'].dt.minute / 60

# Aggregating LDR feature (mean of all LDR readings)
df['LDR_Mean'] = df[['LDR_FL', 'LDR_FR', 'LDR_BL', 'LDR_BR']].mean(axis=1)

# Lag features for voltage output and LDR mean
df['Voltage_Output_Lag1'] = df['Voltage_Output'].shift(1)
df['LDR_Mean_Lag1'] = df['LDR_Mean'].shift(1)

# Droping the first row due to NaN values from shifting
df = df.dropna().reset_index(drop=True)

"""# **Data Transformation**

"""

# Initializing the scaler
scaler = StandardScaler()

# List of features which need to scale
features_to_scale = ['TimeOfDay', 'LDR_Mean', 'LDR_Mean_Lag1']

# Scaling the features using StandardScaler
df[features_to_scale] = scaler.fit_transform(df[features_to_scale])

# Spliting the data into training and testing sets
X = df[features_to_scale + ['Pump_Status_Binary']]
y = df['Voltage_Output']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# **Feature and Target Selection & Dataset Splitting:**


"""

# Regression Task: Predicting Voltage_Output from our Database
X_reg = df[['TimeOfDay', 'LDR_Mean', 'LDR_Mean_Lag1']]
y_reg = df['Voltage_Output']

# Classification Task: For Predicting Pump_Status_Binary
X_class = df[['TimeOfDay', 'LDR_Mean', 'LDR_Mean_Lag1', 'Voltage_Output']]
y_class = df['Pump_Status_Binary']

# Spliting the data for the regression task
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

# Spliting the data for the classification task
X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, y_class, test_size=0.2, stratify=y_class, random_state=42)

"""# **Model Training**

# **01 Regression Model**
"""

# Regression Model

regressor = RandomForestRegressor(n_estimators=100, random_state=42)
regressor.fit(X_train_reg, y_train_reg)
y_pred_reg = regressor.predict(X_test_reg)
rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))
print(f"Regression - Root Mean Squared Error: {rmse}")

# Classification Model

classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train_class, y_train_class)
y_pred_class = classifier.predict(X_test_class)
accuracy = accuracy_score(y_test_class, y_pred_class)
print(f"Classification - Accuracy: {accuracy}")

"""Calculating RMSE for the regression model and generating a confusion matrix for the classification model"""

# Now for RMSE We have trained regression model and we have y_test_reg and y_pred_reg
rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))
print(f"Regression Model - RMSE: {rmse}")

# classification model With  y_test_class and y_pred_class
cm = confusion_matrix(y_test_class, y_pred_class)
print("Classification Model - Confusion Matrix:")
print(cm)

# Detail report on classification performance
class_report = classification_report(y_test_class, y_pred_class)
print("Classification Model - Classification Report:")
print(class_report)

"""# **02 XGBoost**

# **Training and Evaluating XGBoost Models**

**Voltage_Output**
"""

# Initializing the XGBRegressor
xgb_regressor = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Fiting the regressor to the training data
xgb_regressor.fit(X_train_reg, y_train_reg)

# Predicting on the test set
y_pred_reg = xgb_regressor.predict(X_test_reg)

# Evaluating the model
rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))
print(f"Gradient Boosting Regressor - RMSE: {rmse}")

"""**Pump_Status_Binary**"""

# Initializing the XGBClassifier
xgb_classifier = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Fiting the classifier to the training data
xgb_classifier.fit(X_train_class, y_train_class)

# Predicting on the test set
y_pred_class = xgb_classifier.predict(X_test_class)

# Evaluating the model
accuracy = accuracy_score(y_test_class, y_pred_class)
print(f"XGBoost Classifier - Accuracy: {accuracy}")

# Confusion matrix and classification report
cm = confusion_matrix(y_test_class, y_pred_class)
print("Confusion Matrix:")
print(cm)
class_report = classification_report(y_test_class, y_pred_class)
print("Classification Report:")
print(class_report)

"""# **Enhancing XGBoost Model Performance**

Hyperparameter Tuning
"""

# Defining set of parameters to test
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.05, 0.1, 0.2],
    'max_depth': [3, 4, 5]
}

# Initializing the GridSearchCV object
grid_search = GridSearchCV(estimator=xgb_regressor, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')

# Fiting the grid search to the data
grid_search.fit(X_train_reg, y_train_reg)

# The best parameters from Data
print(f"Best parameters: {grid_search.best_params_}")

"""Cross-Validation of XGBoost Regression Model


"""

# Cross-validation scores
scores = cross_val_score(xgb_regressor, X_reg, y_reg, cv=5, scoring='neg_mean_squared_error')
rmse_scores = np.sqrt(-scores)
print(f"Cross-validated RMSE scores: {rmse_scores}")
print(f"Mean RMSE: {rmse_scores.mean()}")

"""# **03 SMOTE**"""

# Creating an instance of SMOTE
smote = SMOTE(random_state=42)

# Defining features and target variable
X_class = df[['TimeOfDay', 'LDR_Mean', 'LDR_Mean_Lag1', 'Voltage_Output']]
y_class = df['Pump_Status_Binary']

# Applying SMOTE
X_smote, y_smote = smote.fit_resample(X_class, y_class)

# Printing updatetd balanced dataset
print('Balanced class distribution:', np.bincount(y_smote))

""" **Split the Data Again for SMOTE**"""

# Spliting the balanced data into training and testing sets
X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(
    X_smote, y_smote, test_size=0.2, random_state=42
)

"""**Trainng classifier on the balanced dataset**"""

# Initializing the classifier
xgb_classifier_smote = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Training the classifier
xgb_classifier_smote.fit(X_train_smote, y_train_smote)

# Making predictions on the test set
y_pred_smote = xgb_classifier_smote.predict(X_test_smote)

# Calculating accuracy and other metrics
accuracy_smote = accuracy_score(y_test_smote, y_pred_smote)
print('Accuracy after SMOTE:', accuracy_smote)

# Generating a confusion matrix and classification report
cm_smote = confusion_matrix(y_test_smote, y_pred_smote)
print('Confusion Matrix after SMOTE:\n', cm_smote)
cr_smote = classification_report(y_test_smote, y_pred_smote)
print('Classification Report after SMOTE:\n', cr_smote)